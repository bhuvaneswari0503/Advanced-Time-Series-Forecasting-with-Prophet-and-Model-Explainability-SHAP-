{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e2b1b-a0f6-4889-a275-2a6d1ec96ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prophet_shap_pipeline.py\n",
    "\n",
    "Full pipeline:\n",
    " - Generate synthetic multiseasonal daily time series with two external regressors (temp, econ)\n",
    " - Fit Prophet with external regressors & holidays + custom seasonalities\n",
    " - Perform rolling-origin cross-validation to evaluate models\n",
    " - Select best model by MAE, serialize model\n",
    " - Run Kernel SHAP (approximate) to explain feature contributions\n",
    " - Save dataset and artifacts (plots, model json, textual report)\n",
    "\n",
    "Requirements (pip install):\n",
    "  pandas numpy matplotlib scikit-learn prophet shap\n",
    "\n",
    "Notes:\n",
    " - Kernel SHAP is computationally expensive; this script uses small nsamples and small explanation subset\n",
    " - Adjust horizons, splits, and nsamples depending on compute/time budget\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.serialize import model_to_json\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Utility metrics\n",
    "# -------------------------\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# -------------------------\n",
    "# 1) Generate synthetic dataset\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "periods = 5 * 365    # 5 years daily\n",
    "start = pd.Timestamp(\"2018-01-01\")\n",
    "dates = pd.date_range(start, periods=periods, freq=\"D\")\n",
    "\n",
    "def yearly_seasonal(t_index):\n",
    "    # t_index: DatetimeIndex or Series of timestamps\n",
    "    return 20 * np.sin(2 * np.pi * (t_index.dayofyear / 365.25))\n",
    "\n",
    "def weekly_seasonal(t_index):\n",
    "    # weekday effect (Mon=0..Sun=6)\n",
    "    return 5 * np.where(t_index.weekday < 5, 1.0, 0.2)\n",
    "\n",
    "# External regressors\n",
    "temp = 10 + 10 * np.sin(2 * np.pi * (dates.dayofyear / 365.25)) + np.random.normal(0, 1.5, size=len(dates))\n",
    "econ = 100 + 0.02 * np.arange(len(dates)) + 2 * np.sin(2 * np.pi * (dates.day / 30.5)) + np.random.normal(0, 0.5, size=len(dates))\n",
    "\n",
    "# Add transient shocks (positive/negative)\n",
    "shock = np.zeros(len(dates))\n",
    "shock_indices = np.random.choice(np.arange(200, len(dates)-200), size=8, replace=False)\n",
    "for idx in shock_indices:\n",
    "    shock[idx: idx+7] += np.random.choice([-15, 20]) * np.exp(-np.linspace(0,1,7)*2)\n",
    "\n",
    "# Holidays sample (India-like important dates; you can expand)\n",
    "holidays = pd.DataFrame({\n",
    "    'ds': pd.to_datetime([\n",
    "        '2018-01-26','2018-08-15','2019-01-26','2019-08-15',\n",
    "        '2020-01-26','2020-08-15','2021-01-26','2021-08-15',\n",
    "        '2022-01-26','2022-08-15'\n",
    "    ]),\n",
    "    'holiday': 'national'\n",
    "})\n",
    "\n",
    "# Compose target y\n",
    "y = 200 + yearly_seasonal(dates) + weekly_seasonal(dates) + 0.8 * temp + 0.05 * econ + shock + np.random.normal(0, 3.0, size=len(dates))\n",
    "df = pd.DataFrame({'ds': dates, 'y': y, 'temp': temp, 'econ': econ})\n",
    "\n",
    "# Create output directory\n",
    "outdir = \"./outputs_prophet_shap\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "csv_path = os.path.join(outdir, \"synthetic_timeseries_prophet.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Rolling-origin CV configuration\n",
    "# -------------------------\n",
    "horizon = 90            # forecast horizon (days)\n",
    "initial_days = 365 * 2  # initial training window (2 years)\n",
    "step = 180              # move forward by 180 days each fold\n",
    "\n",
    "def rolling_origin_splits(df_len, initial_days=365*2, horizon=90, step=180):\n",
    "    cur_train_end = initial_days\n",
    "    splits = []\n",
    "    while cur_train_end + horizon <= df_len:\n",
    "        train_start = 0\n",
    "        train_end = cur_train_end\n",
    "        test_start = cur_train_end\n",
    "        test_end = cur_train_end + horizon\n",
    "        splits.append((train_start, train_end, test_start, test_end))\n",
    "        cur_train_end += step\n",
    "    return splits\n",
    "\n",
    "splits = rolling_origin_splits(len(df), initial_days=initial_days, horizon=horizon, step=step)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Model training & evaluation helper\n",
    "# -------------------------\n",
    "def fit_prophet_with_regressors(train_df, test_df, holidays_df=None, changepoint_prior_scale=0.05):\n",
    "    # Prophet instantiation\n",
    "    m = Prophet(\n",
    "        changepoint_prior_scale=changepoint_prior_scale,\n",
    "        seasonality_mode='additive',\n",
    "        holidays=holidays_df\n",
    "    )\n",
    "    # Add regressors\n",
    "    m.add_regressor('temp')\n",
    "    m.add_regressor('econ')\n",
    "    # Optional: explicitly add custom seasonalities (yearly & weekly)\n",
    "    m.add_seasonality(name='weekly_custom', period=7, fourier_order=10)\n",
    "    m.add_seasonality(name='yearly_custom', period=365.25, fourier_order=20)\n",
    "    # Fit\n",
    "    m.fit(train_df[['ds','y','temp','econ']])\n",
    "    # Prepare future frame for test\n",
    "    future = test_df[['ds','temp','econ']].copy()\n",
    "    forecast = m.predict(future)\n",
    "    return m, forecast\n",
    "\n",
    "# small hyperparameter grid (expand if you want)\n",
    "param_grid = [\n",
    "    {'changepoint_prior_scale': 0.01},\n",
    "    {'changepoint_prior_scale': 0.05},\n",
    "    {'changepoint_prior_scale': 0.2}\n",
    "]\n",
    "\n",
    "cv_results = []\n",
    "models = []\n",
    "\n",
    "for fold_idx, (ts, te, vs, ve) in enumerate(splits):\n",
    "    train_df = df.iloc[ts:te].copy()\n",
    "    test_df = df.iloc[vs:ve].copy()\n",
    "    # cycle through param grid (simple demo); more thorough tuning recommended\n",
    "    best_fold = None\n",
    "    for params in param_grid:\n",
    "        m, forecast = fit_prophet_with_regressors(train_df, test_df, holidays_df=holidays, **params)\n",
    "        y_true = test_df['y'].values\n",
    "        y_pred = forecast['yhat'].values\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mape_v = mape(y_true, y_pred)\n",
    "        if best_fold is None or mae < best_fold['mae']:\n",
    "            best_fold = {\n",
    "                'fold': fold_idx,\n",
    "                'params': params,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape_v,\n",
    "                'model': m,\n",
    "                'train_df': train_df.copy(),\n",
    "                'test_df': test_df.copy(),\n",
    "                'forecast': forecast.copy()\n",
    "            }\n",
    "    cv_results.append(best_fold)\n",
    "    models.append(best_fold)\n",
    "    print(f\"Fold {fold_idx}: MAE={best_fold['mae']:.3f}, RMSE={best_fold['rmse']:.3f}, MAPE={best_fold['mape']:.3f}% | params={best_fold['params']}\")\n",
    "\n",
    "# choose best model across folds by MAE\n",
    "best_idx = int(np.argmin([r['mae'] for r in cv_results]))\n",
    "best = cv_results[best_idx]\n",
    "best_model = best['model']\n",
    "best_train = best['train_df']\n",
    "best_test = best['test_df']\n",
    "best_forecast = best['forecast']\n",
    "print(f\"\\nSelected fold {best_idx} as best by MAE: {best['mae']:.3f}\")\n",
    "\n",
    "# Serialize best model\n",
    "model_outdir = os.path.join(outdir, \"prophet_model\")\n",
    "os.makedirs(model_outdir, exist_ok=True)\n",
    "with open(os.path.join(model_outdir, \"model.json\"), \"w\") as f:\n",
    "    f.write(model_to_json(best_model))\n",
    "\n",
    "# -------------------------\n",
    "# 4) Final metrics on selected fold\n",
    "# -------------------------\n",
    "final_mae = best['mae']\n",
    "final_rmse = best['rmse']\n",
    "final_mape = best['mape']\n",
    "\n",
    "# -------------------------\n",
    "# 5) SHAP explainability (KernelExplainer approximation)\n",
    "# -------------------------\n",
    "# Use two features: temp, econ\n",
    "feature_names = ['temp', 'econ']\n",
    "\n",
    "# background: sample from train regressors (small sample)\n",
    "background = best_train[['temp','econ']].sample(n=min(100, len(best_train)), random_state=1).to_numpy()\n",
    "# explanation set: first N rows of test\n",
    "explain_X = best_test[['temp','econ']].iloc[:30].to_numpy()\n",
    "\n",
    "# define wrapper prediction function that accepts numpy array X (n_samples, n_features)\n",
    "def prophet_predict_from_features(X):\n",
    "    # For each input row, reuse corresponding ds from best_test (first rows) and set regressors\n",
    "    preds = []\n",
    "    for i in range(X.shape[0]):\n",
    "        # take row i's ds from best_test to keep consistent date context\n",
    "        row = best_test.iloc[i:i+1].copy().reset_index(drop=True)\n",
    "        row['temp'] = X[i, 0]\n",
    "        row['econ'] = X[i, 1]\n",
    "        pred = best_model.predict(row[['ds','temp','econ']])\n",
    "        preds.append(pred['yhat'].values[0])\n",
    "    return np.array(preds)\n",
    "\n",
    "# Kernel SHAP (note: slow for large nsamples)\n",
    "explainer = shap.KernelExplainer(prophet_predict_from_features, background)\n",
    "shap_values = explainer.shap_values(explain_X, nsamples=200)  # nsamples small for speed\n",
    "\n",
    "# SHAP feature importance summary (mean absolute)\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "importance = sorted(zip(feature_names, mean_abs_shap), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save a small bar chart of importance\n",
    "plt.figure(figsize=(6,3))\n",
    "names = [n for n,_ in importance][::-1]  # reverse for horizontal bar asc order\n",
    "vals = [v for _,v in importance][::-1]\n",
    "y_pos = range(len(names))\n",
    "plt.barh(y_pos, vals)\n",
    "plt.yticks(y_pos, names)\n",
    "plt.xlabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Feature importance (Kernel SHAP)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"shap_feature_importance.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create an approximate \"force/waterfall\" static plot for the first explained instance\n",
    "def waterfall_plot(base_value, shap_vals_single, feature_names, fname):\n",
    "    # shap_vals_single: array of contributions for each feature for a single prediction\n",
    "    cum = base_value\n",
    "    starts = []\n",
    "    widths = []\n",
    "    labels = []\n",
    "    for i, v in enumerate(shap_vals_single):\n",
    "        starts.append(cum)\n",
    "        widths.append(v)\n",
    "        labels.append(f\"{feature_names[i]} ({v:.2f})\")\n",
    "        cum += v\n",
    "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "    ax.barh([0]*len(widths), widths, left=starts)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Contribution to prediction\")\n",
    "    ax.set_title(\"Approx. SHAP contributions (waterfall) - first explained point\")\n",
    "    ax.legend(labels, loc='upper right', fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "# Estimate expected base value as mean prediction on background\n",
    "expected_value = np.mean(prophet_predict_from_features(background))\n",
    "waterfall_plot(expected_value, shap_values[0], feature_names, os.path.join(outdir, \"shap_force_approx.png\"))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Textual report & summary outputs\n",
    "# -------------------------\n",
    "report_text = []\n",
    "report_text.append(\"=== Prophet + Kernel SHAP Report ===\")\n",
    "report_text.append(f\"Dataset: synthetic_timeseries_prophet.csv  (rows={len(df)})\")\n",
    "report_text.append(\"Model: Prophet with regressors ['temp','econ'] + custom weekly/yearly seasonalities + holidays\")\n",
    "report_text.append(f\"Cross-validation: rolling-origin (initial_days={initial_days}, horizon={horizon}, step={step})\")\n",
    "report_text.append(f\"Number of folds evaluated: {len(splits)}\")\n",
    "report_text.append(f\"Selected fold index (best MAE): {best_idx}\")\n",
    "report_text.append(\"Final test metrics on selected fold:\")\n",
    "report_text.append(f\"  - MAE: {final_mae:.3f}\")\n",
    "report_text.append(f\"  - RMSE: {final_rmse:.3f}\")\n",
    "report_text.append(f\"  - MAPE: {final_mape:.3f}%\")\n",
    "report_text.append(\"\")\n",
    "report_text.append(\"SHAP Summary (mean |SHAP| per feature):\")\n",
    "total = sum([v for _,v in importance]) if len(importance)>0 else 1.0\n",
    "for name, val in importance:\n",
    "    report_text.append(f\"  - {name}: mean |SHAP| = {val:.4f}  (relative {(val/total)*100:.1f}%)\")\n",
    "report_text.append(\"\")\n",
    "report_text.append(\"Interpretation (example):\")\n",
    "report_text.append(\"  - In the example explained test points, the external regressors 'temp' and 'econ' were the primary drivers among regressors.\")\n",
    "report_text.append(\"  - SHAP quantifies marginal contributions to predictions for each explained point. Use shap_feature_importance.png and shap_force_approx.png for quick visual summaries.\")\n",
    "report_text.append(\"\")\n",
    "report_text.append(\"Saved artifacts:\")\n",
    "report_text.append(f\"  - {csv_path}\")\n",
    "report_text.append(f\"  - {os.path.join(model_outdir, 'model.json')}\")\n",
    "report_text.append(f\"  - {os.path.join(outdir, 'shap_feature_importance.png')}\")\n",
    "report_text.append(f\"  - {os.path.join(outdir, 'shap_force_approx.png')}\")\n",
    "\n",
    "report_file = os.path.join(outdir, \"prophet_shap_report.txt\")\n",
    "with open(report_file, \"w\") as fh:\n",
    "    fh.write(\"\\n\".join(report_text))\n",
    "\n",
    "# Print short console summary\n",
    "print(\"\\n\".join(report_text[:20]))\n",
    "print(f\"\\nArtifacts saved under: {outdir}\")\n",
    "\n",
    "# Exit successfully\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
